{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM50268 :: Lab 1 :: Bayesian Linear Regression\n",
    "\n",
    "**Total Marks 20 (20% of overall unit marks).**\n",
    "\n",
    "*Start date: Wednesday February 20, 2019. Submission deadline: 6pm, Friday March 1, 2019.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lab focuses on implementing some of the requisite calculations and manipulations for applying Bayesian linear regression models of the type covered in Lectures 3 and 4.\n",
    "\n",
    "Exercises include:\n",
    "\n",
    "- computing the posterior distribution,\n",
    "- computing the marginal likelihood,\n",
    "- evaluating and choosing regularisation parameters,\n",
    "- obtaining posterior mean models,\n",
    "- calculating error-bars (the predictive variance), and,\n",
    "- ... some presentation of results.\n",
    "\n",
    "As in the lectures, the data will be derived from a synthetic, noisy, sine wave and our model will be a linearly-weighted set of \"Gaussian\" (RBF) basis functions. In a change from the lectures, a modification has been made to the data: there are no observations from one particular region of the data space. One of the aims of the lab is to see how this aspect impacts on the uncertainty of the model predictions.\n",
    "\n",
    "There are 3 principal tasks (detailed below), with varying marks. In summary here:\n",
    "\n",
    "1. Fit various Gaussian RBF-based linear models to the training data using penalised least-squares, and visualise the results (4 marks),\n",
    "2. Replicate slide 15 of Lecture 4 (or, Figure 12 in the \"Week02\" notes) for the model and data here  - that is, compute the train, validation and test set errors, plus the marginal likelihood, over a range of $\\lambda$ (or $\\alpha$) values (11 marks),\n",
    "3. Identify the best posterior mean model, visualise it along with its error-bars (predictive variance), and consider how those error-bars might be usefully exploited (5 marks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission & Marking\n",
    "\n",
    "This lab exercise is assessed, and the marks will contribute to your final grade. For this lab exercise there are a number of places where you are expected to enter your own code. Every place you have to add code is described in the text and specifically indicated by the comment:\n",
    "\n",
    "`#### **** YOUR CODE HERE **** ####`\n",
    "\n",
    "\n",
    "There is also one place where you are asked to enter explanatory text. Full instructions as to what is expected should be found above all the relevant cells.\n",
    "\n",
    "**Please submit your completed workbook using Moodle before 6pm on Friday March 1, 2019**. The workbook you submit must be an `.ipynb` file, which is saved into the directory you're running Jupyter; alternatively you can download it from the menu above using `File -> Download As -> Notebook (.ipynb)`. Remember to save your work regularly (Save and Checkpoint in the File menu, the icon of a floppy disk, or Ctrl-S); the version you submit should have all code blocks showing the results (if any) of execution below them.\n",
    "\n",
    "**You should take care to avoid any suggestion of plagiarism in your submission.** There is helpful information on how to avoid plagiarism on the University website: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Support code\n",
    "\n",
    "To get started, the code cell below imports the requisite standard Python modules, plus a setup module `cm50268_lab1_setup` specific to this lab. This module contains:\n",
    "\n",
    "- the class `DataGenerator` to synthesise all the data sets needed,\n",
    "- the class `RBFGenerator` to create the necessary Gaussian \"RBF\" basis matrices for varying data sets,\n",
    "- the function `error_rms` to simply calculate errors for a given target values $t$ and corresponding model output $y$.\n",
    "\n",
    "We also set some \"constants\" below: data set sizes and the generative noise standard deviation, which we fix at $\\sigma=0.1$ for the entire exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Standard modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# Lab-specific support module\n",
    "import cm50268_lab1_setup as lab1\n",
    "#\n",
    "N_train = 12\n",
    "N_val   = N_train\n",
    "N_test  = 250\n",
    "#\n",
    "sigma = 0.1\n",
    "s2    = sigma**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "We synthesise three data sets:\n",
    "- training set of size $N_{train}=12$ with added noise $\\sigma=0.1$\n",
    "- validation set of size $N_{val}=12$ with added noise $\\sigma=0.1$\n",
    "- test set of size $N_{test}=250$ with **no noise** and which **covers the full $x$-space**\n",
    "\n",
    "**Note:** we assume throughtout that $\\sigma$ is known and fixed. A feature of the test set here is that *it will include data from a region where there is no training data*.\n",
    "\n",
    "### Generate Basis\n",
    "For our linear model, we use as many functions as data points (a \"complete\" basis), comprising $N-1$ equally-spaced Gaussian functions (of width 1), plus a fixed \"bias\" or \"offset\". If we call `evaluate` on the basis generator, we get a $N\\times{}M$ matrix $\\mathbf{\\Phi}$ returned, where each column / row contains the output of each basis function on each data point respectively: that is, $\\mathbf{\\Phi}_{nm} = \\phi_m(x_n)$. The use of a bias means the first column contains simply a fixed value of one. \n",
    "\n",
    "For the training and validation set, this matrix $\\mathbf{\\Phi}$ will be $12\\times 12$, whereas for the test set it will be $250 \\times 12$.\n",
    "\n",
    "For illustration, the data and the underlying \"truth\" (sine wave) are shown below, with basis functions overlaid.\n",
    "\n",
    "<img src=\"data-and-basis.png\" alt=\"Data and basis\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data - create generator instance, and synthesise 3 sets\n",
    "#\n",
    "generator = lab1.DataGenerator(noise=sigma)\n",
    "#\n",
    "(x_train, t_train) = generator.get_data('TRAIN', N_train)\n",
    "(x_val, t_val) = generator.get_data('VALIDATION', N_val)\n",
    "(x_test, t_test) = generator.get_data('TEST', N_test)\n",
    "\n",
    "# Basis - create generator instance and compute the basis matrices for all 3 data sets\n",
    "# Note that because we use a \"bias\" function, we need N-1 Gaussians to make the\n",
    "# basis \"complete\" (i.e. for M=N)\n",
    "#\n",
    "M = N_train-1\n",
    "r = 1 # Basis radius or width\n",
    "centres = np.linspace(generator.xmin, generator.xmax, M)\n",
    "basis = lab1.RBFGenerator(centres, width=r, bias=True)\n",
    "#\n",
    "PHI_train = basis.evaluate(x_train)\n",
    "PHI_val = basis.evaluate(x_val)\n",
    "PHI_test = basis.evaluate(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a \n",
    "**(2 marks)**\n",
    "\n",
    "Write a function `fit_pls` (its signature is defined in the cell below) to fit a linear model with basis matrix `PHI_train` to the training data `t_train` for a given value of regularisation parameter $\\lambda$. It should return the weight vector $\\mathbf{w}_{PLS}$ that minimises the penalised least squares error.\n",
    "\n",
    "You may find the functions `np.linalg.lstsq` and/or `np.linalg.inv` applicable.\n",
    "\n",
    "The ideal solution will treat the $\\lambda=0$ case differently to $\\lambda>0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIT_PLS\n",
    "##\n",
    "def fit_pls(PHI, t, lam):\n",
    "    #\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    #\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b\n",
    "**(2 marks)**\n",
    "\n",
    "First, write a convenient graphing function `plot_regression` that you will need for this and following tasks. It should be capable of being passed suitable arguments to plot on the same axes:\n",
    "\n",
    "- the data (*i.e.* `t_train` above),\n",
    "- the generating function (*i.e.* the noise-free `t_test` above),\n",
    "- some approximating function, or predictor, $y$ (you will compute some shortly).\n",
    "\n",
    "A simple example of output is shown below. You are welcome to choose your own consistent styles (indeed, are encouraged to!), and need not follow mine.\n",
    "\n",
    "<img src=\"regression.png\" alt=\"Data and Predictor\" style=\"width: 500px;\"/>\n",
    "\n",
    "Using `fit_pls` in conjunction with `plot_regression`, add code below to plot three graphs for values of $\\lambda$ in \\[ 0, 0.01, 10\\]. In each graph (appropriately labelled by $\\lambda$), show:\n",
    "\n",
    "1. the training data (`x_train` / `t_train`)\n",
    "2. the underlying generating function (by plotting `x_test` / `t_test`)\n",
    "3. your fitted function at points `x_test` (you will need to multiply the relevant $\\mathbf{\\Phi}$ by $\\mathbf{w}_{PLS}$)\n",
    "\n",
    "You should find that the first $\\lambda$ value over-fits, the second is \"about right\" (but only where there is data!) and the third under-fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a\n",
    "**(2 marks)**\n",
    "\n",
    "Write a function `compute_posterior` to compute the posterior mean $\\mathbf{\\mu}$ and covariance $\\mathbf{\\Sigma}$ for the Bayesian linear regression model with basis matrix $\\mathbf{\\Phi}$ and with hyperparameters $\\alpha$ and $\\sigma^2$.\n",
    "\n",
    "Verify the consistency of your posterior code with `fit_pls` by comparing the outputs `w` and `Mu` (they should be the same, remembering that for equivalence, $\\alpha = \\lambda/\\sigma^2$). As well as defining your function below, append a few lines of code underneath which show your consistency check for $\\lambda=0.01$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POSTERIOR\n",
    "##\n",
    "def compute_posterior(PHI, t, alph, s2):\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    \n",
    "    return Mu, SIGMA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b\n",
    "**(3 marks)**\n",
    "\n",
    "Write a function `compute_log_marginal` to compute the *logarithm* of the marginal likelihood for the Bayesian linear regression model with basis matrix $\\mathbf{\\Phi}$ and with hyperparameters $\\alpha$ and $\\sigma^2$.\n",
    "\n",
    "In principle, you may wish to use `stats.multivariate_normal.logpdf` (don't compute the pdf and then call `np.log`!).\n",
    "\n",
    "In practice, you should not have numerical issues using `stats.multivariate_normal.logpdf` (this can happen if you experiment with larger data sets, in which case you may wish to look at the `allow_singular` argument.)\n",
    "\n",
    "There is an alternative, and more robust, way of calculating the marginal likelihood directly, which you may wish to explore if you are comfortable with linear algebra and matrix identities. (**1 mark** reserved for this.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MARGINAL LIKELIHOOD\n",
    "##\n",
    "def compute_log_marginal(PHI, t, alph, s2):\n",
    "    #### **** YOUR CODE HERE **** ####\n",
    "    \n",
    "    return lgp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2c\n",
    "**(4 marks)**\n",
    "\n",
    "In the cell below, write some code to replicate slide 15 of Lecture 4 for the Gaussian RBF basis and the `t_train` data defined above. Here is a reminder of the relevant graph:\n",
    "\n",
    "<img src=\"slide15.png\" alt=\"Slide 15 from Lecture 4\" style=\"width: 400px;\"/>\n",
    "\n",
    "In more detail, over a range of $\\lambda$ (or equivalent $\\alpha=\\lambda/\\sigma^2$) values, you should:\n",
    "\n",
    "- Compute the train, validation and test set errors for the penalised least-squares model (use your `fit_pls` to fit it, and the supplied `error_rms` to calculate the error). \n",
    "- Also compute the *negative* log marginal likelihood, using the function just written.\n",
    "- Plot all these curves on the same graph, noting that the scale for the errors is different for that of the negative log marginal likelihood.\n",
    "- In addition to plotting the graph below, add code to compute, and output (using `print`), the *test error* corresponding to the minimum point on: (1) the test error curve itself, (2) the validation curve, (3) the negative marginal likelihood curve.\n",
    "\n",
    "For easiest interpretation, I would suggest defining your $\\lambda$ range logarithmically. First use `np.linspace(begin,end,steps)` to create equally spaced values `v`, then specify `lambda = 10**v`. For `steps`, a value of 100 would work well.\n",
    "\n",
    "You should find that your graph has a similar qualitative form to that in the lecture slides (and shown above), although the test error curve looks rather \"sharp\".\n",
    "\n",
    "**Hints:** \n",
    "- You can create a new $y$-axis on the same plot, which shares the $x$-axis, using `plt.gca().twinx()`.\n",
    "- Remember that $\\alpha=\\lambda/\\sigma^2$. If you don't rescale appropriately, the marginal likelihood curve will not be correctly aligned with the error curves and your minimum point will be incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2d\n",
    "**(2 marks)**\n",
    "\n",
    "Find the best value of $\\alpha$ according to the marginal likelihood in Task 2c above, call `compute_posterior` to find the posterior mean weight vector `Mu` and use this to compute the posterior mean predictor (*i.e.* $y=\\Phi\\mu$) at all the test points `x_test`. Then, similar to Task 1b, using your earlier `plot_regression` function, plot on the same axes:\n",
    "1. the training data (`x_train` / `t_train`),\n",
    "2. the underlying generating function (by plotting `x_test` and `t_test`),\n",
    "3. the posterior mean predictor function you just calculated.\n",
    "\n",
    "You should find that the predictor is a pretty good fit to the underlying generating function *where there was data in the training set*. We can't expect the model to make good predictions in the \"gap\" where it has not seen any training data (unless it gets lucky!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3a\n",
    "**(3 marks)**\n",
    "\n",
    "Repeat the graph above (Task 2d), for the same \"best\" predictor, but this time also compute the predictive variance at the points `x_test` and add them to the same plot as \"error bars\".\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- By \"error bars\" we mean some multiple of the predictive standard deviation, and a multiple of 1 is fine, though other choices (*e.g.* 1.96 or 2.0) can be applicable,\n",
    "- To compute the predictive variance at `x_test`, you'll need the matrix `PHI_test`,\n",
    "- The matplotlib function `plt.fill_between` is a handy utility for plotting error bars (though you don't have to use it),\n",
    "- See Figures 16 and 17 from the \"Week02\" notes for examples,\n",
    "- If your calculations are correct, you should expect to see larger error bars where there is no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3b\n",
    "**(2 marks)**\n",
    "\n",
    "This is a more open-ended question, with an opportunity to think more creatively. (Note that there are only two marks available though.)\n",
    "\n",
    "Imagine you had built the above model for inclusion within some real-world system where it was desired to make predictions at some arbitrary points $x$ in the future. How might the information from your model's predictive variance be useful in the practical application?\n",
    "\n",
    "Remember that the predictive model is an $x$-dependent Gaussian distribution over the prediction, and in principle the variance quantifies the likely error (which is a consequence both of the noise on the data and any model uncertainty).\n",
    "\n",
    "More specifically, your answer might look to address the following questions:\n",
    "\n",
    "- how could the system exploit the predictive variance to practical advantage on new data?\n",
    "- what evidence can you supply now (based on the existing test data set) to suggest the approach might work?\n",
    "\n",
    "Explain what you might do in the text box below (no more than 200 words is suggested), and in the following box, generate whatever graph(s) you would offer as supporting evidence for the utility of your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the predictive variance in practice\n",
    "\n",
    "**Add your explanation here.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### **** YOUR CODE HERE **** ####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
